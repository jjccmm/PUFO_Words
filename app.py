import pandas as pd
import streamlit as st
import plotly.graph_objs as go
import json


st.set_page_config(layout="wide")

# ğŸ“¥ CSV laden und umstrukturieren
@st.cache_data
def load_data():
    df = pd.read_csv("word_counts.csv", index_col=0)
    df.fillna(0, inplace=True)  # NaN-Werte durch 0 ersetzen
    df = df[df["is_stop"] == False]  # nur relevante WÃ¶rter
    df = df.drop(columns=["is_stop"])
    df = df.astype("uint8")
    df = df.T  # Episoden = Zeilen
    df.index.name = "Episode"
    df.reset_index(inplace=True)
    return df


@st.cache_data
def load_stats():
    with open("episode_stats.json", "r", encoding="utf-8") as f:
        stats =  json.load(f)
    episodes_stats_df = pd.DataFrame(stats["episodes"])
    return stats, episodes_stats_df
    

df = load_data()
stats, episodes_stats_df = load_stats()

st.title("ğŸ™ï¸ Die Das-Podcast-Ufo Podcast-Wortanalyse")

# ğŸ¯ WÃ¶rter-Auswahl
word_columns = df.columns.drop("Episode")
selected_words = st.multiselect("ğŸ” WÃ¤hle WÃ¶rter", word_columns, default=['eimer', 'mÃ¼nze', 'cent']) 

if selected_words:
    df_selected = df[["Episode"] + selected_words]
    
    # ğŸ“ˆ HÃ¤ufigkeit Ã¼ber Episoden (Stacked Line Plot)
    st.subheader("ğŸ“Š HÃ¤ufigkeit der gewÃ¤hlten WÃ¶rter Ã¼ber alle Episoden")
    fig_line = go.Figure()
    for word in selected_words:
        fig_line.add_trace(go.Scatter(
            x=df_selected["Episode"], y=df_selected[word], mode="lines", stackgroup="one", name=word
        ))
    fig_line.update_layout(
        xaxis_title="Episode",
        yaxis_title="Anzahl",
        width=1000,
        height=400
    )
    st.plotly_chart(fig_line, use_container_width=False)

    # ğŸ† Top 10 Episoden mit hÃ¶chster Wortanzahl
    st.subheader("ğŸ… Top 10 Episoden mit hÃ¤ufigster Verwendung ausgewÃ¤hlter WÃ¶rter")

    df_selected["total_selected"] = df_selected[selected_words].sum(axis=1)

    # Top 10 nach HÃ¤ufigkeit
    top10 = df_selected.sort_values("total_selected", ascending=False).head(10)

    # Episoden als Strings fÃ¼r korrektes Y-Achsen-Labeling
    top10["Episode_str"] = "Episode " + top10["Episode"].astype(str)

    # Balken umgekehrt sortieren (damit hÃ¤ufigste oben steht)
    top10_sorted = top10.sort_values("total_selected", ascending=True)

    fig_bar = go.Figure()
    for word in selected_words:
        fig_bar.add_trace(go.Bar(
            x=top10_sorted[word],
            y=top10_sorted["Episode_str"],
            name=word,
            orientation="h"
        ))

    fig_bar.update_layout(
        barmode="stack",
        xaxis_title="Anzahl",
        yaxis_title="Episode (nur Top 10)",
        width=1000,
        height=500,
        yaxis=dict(type="category")  # â¬…ï¸ Y-Achse kategorisch
    )

    st.plotly_chart(fig_bar, use_container_width=False)

else:
    st.info("â¬† Bitte wÃ¤hle oben ein oder mehrere WÃ¶rter.")

# ğŸ“Š Allgemeine Statistik
st.header("ğŸ“‹ Allgemeine Statistik")

# 1. Gesamtanzahl Episoden
total_episodes = len(df)
# 2. Gesamtanzahl WÃ¶rter
total_words = df[word_columns].sum().sum()
# 3. Anzahl unterschiedlicher WÃ¶rter
unique_words_total = len(word_columns)

col1, col2, col3 = st.columns(3)
col1.metric("ğŸ§ Anzahl Episoden", stats["total_episodes"])
col2.metric("ğŸ—£ï¸ Gesprochene WÃ¶rter insgesamt", f"{stats['total_words']:,}".replace(',', '.'))
col3.metric("ğŸ”¤ Verschiedene WÃ¶rter insgesamt", f"{stats['total_unique_words']:,}".replace(',', '.'))

# WÃ¶rter pro Episode
st.subheader("ğŸ“ˆ WÃ¶rter pro Episode")
fig_total = go.Figure(go.Scatter(
    x=episodes_stats_df["episode"],
    y=episodes_stats_df["total_words"],
    mode="lines",
    name="WÃ¶rter pro Episode"
))
fig_total.update_layout(
    xaxis_title="Episode",
    yaxis_title="WÃ¶rter",
    width=1000,
    height=300
)
st.plotly_chart(fig_total, use_container_width=False)

# Verschiedene WÃ¶rter pro Episode
st.subheader("ğŸ”  Verschiedene WÃ¶rter pro Episode")
fig_unique = go.Figure(go.Scatter(
    x=episodes_stats_df["episode"],
    y=episodes_stats_df["unique_words"],
    mode="lines",
    name="Einzigartige WÃ¶rter"
))
fig_unique.update_layout(
    xaxis_title="Episode",
    yaxis_title="Anzahl",
    width=1000,
    height=300
)
st.plotly_chart(fig_unique, use_container_width=False)

# Neue WÃ¶rter pro Episode
st.subheader("ğŸ†• Neue WÃ¶rter pro Episode")
fig_new = go.Figure(go.Scatter(
    x=episodes_stats_df["episode"],  
    y=episodes_stats_df["new_words"],
    mode="lines",
    name="Neue WÃ¶rter"
))
fig_new.update_layout(
    xaxis_title="Episode",
    yaxis_title="Neue WÃ¶rter",
    yaxis_type="log",
    title="Neue WÃ¶rter pro Episode (logarithmisch)",
    width=1000,
    height=300
)
st.plotly_chart(fig_new, use_container_width=False)


with st.expander("ğŸ”§ So wurde die Analyse erstellt"):
    st.markdown("""
    Die Analyse der Podcast-Episoden erfolgte in mehreren Schritten:

    1. ğŸ“¡ **RSS-Feed auslesen** â€“ um Links zu den MP3-Dateien der Episoden zu erhalten  
    2. ğŸ’¾ **MP3 herunterladen** â€“ jede Episode wurde lokal gespeichert  
    3. ğŸ§  **Spracherkennung mit Whisper** â€“ die Audiodateien wurden mit dem Whisper-Modell in Text umgewandelt  
    4. âœ‚ï¸ **Tokenisierung mit spaCy** â€“ der transkribierte Text wurde in WÃ¶rter (Lemmata) zerlegt  
    5. ğŸš« **StoppwÃ¶rter entfernen** â€“ hÃ¤ufige WÃ¶rter wie â€undâ€œ, â€oderâ€œ, â€dasâ€œ wurden ausgefiltert  

    **âš ï¸ Herausforderungen:**

    - ğŸ”‰ Fehler bei der automatischen Spracherkennung (z.â€¯B. schlechte AudioqualitÃ¤t, Dialekte)
    - ğŸ§¾ Uneinheitliche oder falsche Lemmatisierung durch NLP-Tools
    """)